{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis using LSTM neural network and Pytorch"
      ],
      "metadata": {
        "id": "7bGIFqJl1HoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo --quiet\n",
        "!pip install shap --quiet\n",
        "!pip install gensim --quiet\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Q1Wf_rJ01G4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91f0359-95c2-42a8-a9c4-51f14ad30499"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/540.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m532.5/540.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json, collections, time, re, string, os,  sys, random, sklearn, shap\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "from torch.utils.data import Dataset\n",
        "from torch import optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "11FH2cv202-P"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "2vGadJFy10W8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reviews = pd.read_csv('https://www.dropbox.com/scl/fi/vdncgb0i4uipvnptdhg6h/imdb_reviews.csv?rlkey=n2iy9s7o750l9wodonjm00u41&st=8jrm7kub&dl=1', index_col = 0 )"
      ],
      "metadata": {
        "id": "ntKFDkWXXnel"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reviews = df_reviews.sample(50000).reset_index(drop=True)\n",
        "df_reviews['label'] = df_reviews['label'].map({'pos': 1,'neg': 0})\n",
        "display(df_reviews['label'].value_counts())\n",
        "\n",
        "print(len(df_reviews))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "myFZRXlzYs9b",
        "outputId": "fe283785-4e6b-4145-8b73-8b7556afeaf4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "label\n",
              "1    25000\n",
              "0    25000\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>25000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = df_reviews.label\n",
        "reviews = df_reviews.review\n",
        "x_train_token, x_test_token, y_train_token, y_test_token = train_test_split(reviews, label, test_size=0.2, train_size=0.8, random_state=42)\n"
      ],
      "metadata": {
        "id": "kpqFfyRuND9r"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create vocabulary of words in text"
      ],
      "metadata": {
        "id": "rrqZHVyP12av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and create vocab\n",
        "tokenized_text = [word_tokenize(text.lower()) for text in x_train_token]              # Tokenize - List of words for each text\n",
        "word_counts = Counter([word for text in tokenized_text for word in text])             # Count of each word\n",
        "\n",
        "vocab = {word: idx + 1 for idx, (word, _) in enumerate(word_counts.most_common())}    # Map word -> index\n",
        "index_word = {idex: word for word, idex in vocab.items()}                             # Map index -> word\n",
        "max_len = max([len(word_tokenize(text.lower())) for text in reviews])                 # Find the longest text in the dataset so it can be used for padding\n",
        "vocab['<PAD>'] = 0\n",
        "\n",
        "def vectorize_text(tokenized_text, max_len):\n",
        "  sequence_padded = []\n",
        "  sequence_length = []\n",
        "\n",
        "  text_sequences = [[vocab.get(word, 0) for word in text] for text in tokenized_text] # Convert text to sequences of indices\n",
        "\n",
        "  # Pad sequences to the same length\n",
        "  for seq in text_sequences:\n",
        "    sequence_padded.append(seq + [0] * (max_len - len(seq)))                         # Pad with zero all sequences so they have the same length\n",
        "    sequence_length.append(len(seq))                                                 # Store the non-padded (real) length of each sequence\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  sequence_padded = torch.tensor(sequence_padded, dtype=torch.long)\n",
        "  sequence_length = torch.tensor(sequence_length, dtype=torch.long)\n",
        "  return sequence_padded, sequence_length\n",
        "\n",
        "x_train, x_train_len = vectorize_text(tokenized_text, max_len)\n",
        "x_test, x_test_len = vectorize_text([word_tokenize(text.lower()) for text in x_test_token], max_len)\n",
        "\n",
        "y_train = torch.tensor(y_train_token.values, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test_token.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "R8PRPT0d1mKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):                                                       # Extends Pytorch Dataset class\n",
        "    def __init__(self, data_dict, label_list, x_train_len):\n",
        "        self.data = data_dict.to(torch.int64)\n",
        "        self.labels = label_list\n",
        "        self.length = x_train_len\n",
        "    def __len__(self): return(len(self.labels))\n",
        "    def __getitem__(self, idx): return (self.data[idx], self.labels[idx], self.length[idx])"
      ],
      "metadata": {
        "id": "6Xf1pG8F96y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(x_train,  y_train, x_train_len)                       # Convert to datatype used by Pytorch\n",
        "val_dataset = TextDataset(x_test,  y_test, x_test_len)                            # Convert to datatype used by Pytorch\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "iQs3F0FO-cRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define model and training algorithm"
      ],
      "metadata": {
        "id": "wmwSwqVa2JJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define model"
      ],
      "metadata": {
        "id": "vnkQvgsN2Sum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx, embed_layer = None):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        if embed_layer is None: self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx) # if pre-trained embedding isnt provided, word embedding is learned\n",
        "        else : self.embedding = nn.Embedding.from_pretrained(embed_layer, freeze=False)     # if pre-trained embedding has been provided\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        embedded = self.embedding(text)\n",
        "        text_lengths = text_lengths.to(torch.int64).to('cpu')\n",
        "        packed_embedded = pack_padded_sequence(embedded, text_lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
        "        # Use only the final hidden state for classification\n",
        "        hidden = hidden[-1, :, :]\n",
        "        return self.fc(hidden)\n"
      ],
      "metadata": {
        "id": "TDu9bUf51rCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define training"
      ],
      "metadata": {
        "id": "_Z6GGWw62Ucj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, N_EPOCHS = 10, device = 'cpu'):\n",
        "  optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay = 5e-4)\n",
        "  criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "\n",
        "  for i, epoch in enumerate(range(N_EPOCHS)):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        encoded_text = batch[0].to(device)\n",
        "        lengths = batch[2]\n",
        "        label = batch[1].to(device)\n",
        "\n",
        "        predictions = model(encoded_text, lengths).squeeze(1)\n",
        "        loss = criterion(predictions, label.float())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    y_hat_all = []\n",
        "    y_true_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "          encoded_text = batch[0].to(device)\n",
        "          lengths = batch[2]\n",
        "          label = batch[1].to(device)\n",
        "          predictions = model(encoded_text, lengths).squeeze(1)\n",
        "          y_hat = (torch.sigmoid(predictions)>.5) *1\n",
        "          #print(accuracy_score(label, y_hat))\n",
        "\n",
        "          y_hat_all.extend([int(x) for x in y_hat])\n",
        "          y_true_all.extend([int(x) for x in label])\n",
        "\n",
        "\n",
        "        print(f\"Accuracy for epoch : {i} :\", accuracy_score(y_true_all, y_hat_all))\n",
        "\n"
      ],
      "metadata": {
        "id": "kAev-97QUWWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model instance, train model and calculate accuracy"
      ],
      "metadata": {
        "id": "lSGprdEVrQUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "hidden_dim = 256\n",
        "output_dim = 1  # Binary classification, 1 for positive sentiment, 0 for negative sentiment\n",
        "pad_idx = 0     # Assuming 0 is used for padding index in vocabulary\n",
        "\n",
        "# Create model instance\n",
        "model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Training on: \", device)\n",
        "\n",
        "model.to(device)\n",
        "train_model(model, train_loader, val_loader, N_EPOCHS = 5, device = device)"
      ],
      "metadata": {
        "id": "DYu-XlgzyesZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d142117-e979-4388-ebcd-4b30c8c1b8b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on:  cuda\n",
            "Accuracy for epoch : 0 : 0.7029\n",
            "Accuracy for epoch : 1 : 0.8625\n",
            "Accuracy for epoch : 2 : 0.8781\n",
            "Accuracy for epoch : 3 : 0.887\n",
            "Accuracy for epoch : 4 : 0.9019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training model using pretrained embedding"
      ],
      "metadata": {
        "id": "kOf5Ghc-2mci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "w2v_model = api.load(\"glove-twitter-25\")\n",
        "\n",
        "# Initialize blank matrix of  Dim = (size of vocabulary, embedding vector dim)\n",
        "embedding_matrix = torch.zeros(len(vocab),w2v_model.vector_size)\n",
        "print(embedding_matrix.shape)\n",
        "\n",
        "# Embbed the vector of each word into the empty matrix just initialized\n",
        "for i, token in enumerate(range(len(vocab))):\n",
        "    embedding_matrix[i] = torch.from_numpy(w2v_model[token])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ShBVejLvxqK",
        "outputId": "416ecf18-716e-4439-aee9-31b138c132f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
            "torch.Size([145822, 25])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-ab953ec4db98>:10: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "  embedding_matrix[i] = torch.from_numpy(w2v_model[token])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx, embedding_matrix)\n",
        "pre_trained_model.to(device)\n",
        "train_model(model, train_loader, val_loader, N_EPOCHS = 5, device = device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVRwR1PE1n6c",
        "outputId": "290b6d56-11b6-4492-8068-dfa2dff95533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for epoch : 0 : 0.9047\n",
            "Accuracy for epoch : 1 : 0.9036\n",
            "Accuracy for epoch : 2 : 0.9062\n",
            "Accuracy for epoch : 3 : 0.9042\n",
            "Accuracy for epoch : 4 : 0.9057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using pre-trained embeddings achieves high accuracy while training on a fewer number of epochs"
      ],
      "metadata": {
        "id": "znxTAmHX90Df"
      }
    }
  ]
}